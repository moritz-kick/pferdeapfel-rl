trainer:
  total_timesteps: 10000000
  learning_rate: 0.0003
  n_steps: 256
  batch_size: 64
  n_envs: 32
  gamma: 0.99
  tensorboard_log: "data/logs/rl"
  save_path: "data/models/ppo_pferdeapfel"
  ent_coef: 0.02
  self_play:
    enabled: true
    sync_interval: 500000   # timesteps between opponent evaluations
    eval_episodes: 128       # games per eval (per pair in the tournament)
    opponent_deterministic: true
    best_opponent_prob: 0.4   # 40%: face current best snapshot
    old_opponent_prob: 0.3    # 30%: face a random old snapshot (max 4 kept)
env:
  agent_color: "random"
  opponent_policy: "self_play"
  opponent_deterministic: false
  random_opponent_chance: 0.30   # with self-play, use a random opponent for ~30% of games to add variance
